{"cells": [{"metadata": {}, "source": "<img src=\"https://ibin.co/3hXAuMuHhEeK.png\" alt=\"Cisco Systems\" width=\"200\" height=\"600\"\nstyle=\"float:left;width:372px;height:118px;\">", "cell_type": "markdown"}, {"metadata": {}, "source": "# Custom alert\n## This User App alerts when unauthorized services servers are used\n### It looks for other servers providing the same service, but not listed in the official list\n### Returns rogue provider and misconfigured consummers\n### It is expected to be scheduled every hour", "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "execution_count": null, "outputs": [], "source": "\"\"\"\nCopyright (c) 2018 Cisco and/or its affiliates.\nThis software is licensed to you under the terms of the Cisco Sample\nCode License, Version 1.0 (the \"License\"). You may obtain a copy of the\nLicense at\n               https://developer.cisco.com/docs/licenses\nAll use of the material herein must be in accordance with the terms of\nthe License. All rights not expressly granted by the License are\nreserved. Unless required by applicable law or agreed to separately in\nwriting, software distributed under the License is distributed on an \"AS\nIS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\nor implied.\n\"\"\"\n\n__author__ = \"Damien Gouju\"\n__copyright__ = \"Copyright (c) 2018 Cisco and/or its affiliates.\"\n__license__ = \"Cisco Sample Code License, Version 1.0\"", "cell_type": "code"}, {"metadata": {"collapsed": false}, "execution_count": null, "outputs": [], "source": "# pylint: disable=invalid-name\n# Import required libs\nimport json\nfrom datetime import datetime\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\n# Configuration\n# Kafka TAP to send alert to\n# Keep it empty if you don't want to send an alert and just get result\nkafka_target = \"Kafka-TAP\"\n\n# To identify the service in the alert message\nservice_type = \"DNS\"\n\n# Datalake directory where the official list is uploaded using \"Data platform --> Data Lake\" in the shared zone\ndirectory = \"/shared/services-list/dns\"\n\n# CSV file must be single column, with this header. Ex of a valid CSV file (without \"\"\" btw):\n\"\"\"\ndns_servers\n8.8.8.8\n192.168.0.1\n\"\"\"\nfield_name = \"dns_servers\"\n\n# Port of the service (53 for DNS, 123 for NTP, ...)\nport = \"53\"\n\n# Procotol, UDP vs TCP\nprotocol = \"UDP\"", "cell_type": "code"}, {"metadata": {"collapsed": false}, "execution_count": null, "outputs": [], "source": "# Loading latest version of the valid DNS servers list\n# Datalake directory\n\nlatest_version = sc._jvm.com.tetration.apps.IO.list(sqlContext._ssql_ctx, directory, \"CSV\")[-1]\nprint(\"Latest version: \"+latest_version)\ndns_list = sc._jvm.com.tetration.apps.IO.read(sqlContext._ssql_ctx, directory, \"CSV\", latest_version)\ndns_list.registerTempTable(\"valid_providers\")\n\nvalid_providers_dataset = sqlContext.sql(\"SELECT * FROM valid_providers\").toPandas()\nsqlContext.dropTempTable(\"valid_providers\")\nprint(\"Valid providers:\")\nprint(valid_providers_dataset)", "cell_type": "code"}, {"metadata": {"collapsed": false}, "execution_count": null, "outputs": [], "source": "# Look for last hour data\nlasthour_data = sc._jvm.com.tetration.apps.IO.read(sqlContext._ssql_ctx, \"/tetration/flows/\", \"PARQUET\", \"LASTHOUR\")\nlasthour_data.registerTempTable(\"flows1h\")\n\n# Look for Consumers / Providers couples that match:\n# - match the port\n# - match the protocol\n# - not match the valid list of providers\n# and group by unique consumer / provider couples\n# We don't care here about the amount of flows\n\n# Building the SQL query\nfirst = 1\nfor server in valid_providers_dataset.iterrows():\n    if first:\n        where_clause = \" dst_address = \\'\"+server[1][field_name]+\"\\' \"\n        first = 0\n    else:\n        where_clause = where_clause + \"OR dst_address = \\'\"+server[1][field_name]+\"\\' \"\n#print(where_clause)\n\nincorrect_dns_flows_dataset = sqlContext.sql(\"SELECT src_address as Consumer, dst_address as Provider FROM flows1h WHERE dst_port = \\'\"+port+\"\\' AND proto = \\'\"+protocol+\"\\' AND NOT (\"+where_clause+\") GROUP BY src_address, dst_address\").toPandas()\nsqlContext.dropTempTable(\"flows1h\")\n#print(incorrect_dns_flows_dataset.to_string(index=False))\nprint(\"Unofficial \"+service_type+\" providers: \"+str(incorrect_dns_flows_dataset['Provider'].unique()))\nprint(\"Badly configured consumers: \"+str(incorrect_dns_flows_dataset['Consumer'].unique()))", "cell_type": "code"}, {"metadata": {"collapsed": false}, "execution_count": null, "outputs": [], "source": "#print(incorrect_dns_flows_dataset.to_json(orient='records'))\n\n# If there are such flows\nif not incorrect_dns_flows_dataset.empty and kafka_target != \"\":\n    kafka_json = json.loads('{\"src_dst\":'+incorrect_dns_flows_dataset.to_json(orient='records')+'}')\n\n    # Tag message with categories\n    kafka_json.update({\"message_type\":\"alert\"})\n    kafka_json.update({\"message_desc\":\"rogue \"+service_type+\" servers usage\"})\n    kafka_json.update({\"message_timestamp\": (datetime.today()).strftime('%Y-%m-%d %H:%M:%S')+\" UTC\"})\n\n    # Send to Kafka\n    sc._jvm.com.tetration.apps.DataTaps.sendMessage(json.dumps(kafka_json), kafka_target)\n    \n    print(\"Message sent to Kafka server \"+kafka_target)\nelse:\n    print(\"No message sent to Kafka\")", "cell_type": "code"}, {"metadata": {"collapsed": true}, "execution_count": null, "outputs": [], "source": "", "cell_type": "code"}], "metadata": {"language_info": {"pygments_lexer": "ipython3", "file_extension": ".py", "mimetype": "text/x-python", "version": "3.5.2", "nbconvert_exporter": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python"}, "kernelspec": {"language": "python", "display_name": "PySpark", "name": "h4_pyspark"}}, "nbformat": 4, "nbformat_minor": 1}